{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMz2XfgGuXbAjjWN3/kSIe7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Notebook: Hugging Face Pre-Trained Tokenizer Comparison\n",
        "# Author: Thomas Purk\n",
        "# Date: 2025-04-02\n",
        "# Reference: https://huggingface.co/docs/tokenizers/index"
      ],
      "metadata": {
        "id": "xqfCuXsCLEPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face Pre-Trained Tokenizer Comparison\n",
        "\n",
        "This notebook contains sample code comparing the major type of text tokenizers\n",
        "used to process text for NLP tasks. A set of five sample sentences is processed\n",
        "by each tokenizer and then printed to an output cell for side by side comparison.\n",
        "\n",
        "Tokenizers:\n",
        "- Word-Based (Hugging Face BasicTokenizer class)\n",
        "- Whitespace (Python's 'text'.split() function)\n",
        "- Character-Based (Python's list('text') function)\n",
        "- BPE Subword (Pre-Trained by the gpt2 model)\n",
        "- Wordpiece Subword (Pre-Trained by the bert-base-cased model)\n",
        "- Unigram Subword (Pre-Trained by the t5-small model)"
      ],
      "metadata": {
        "id": "_FvUksZ3U2YV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "l1cyNkvGDX4K"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a set of strings to test each type of tokenizer\n",
        "sentences = [\n",
        "    \"I can't believe it's already April!\",  # Tests handling of contractions\n",
        "    \"The quick brown fox jumps over the lazy dog.\",  # Classic sentence for testing full-word tokenization\n",
        "    \"Artificial intelligence (AI) is evolving rapidly!\",  # Tests handling of parentheses and abbreviations\n",
        "    \"Unbelievably, the anti-establishment movement gained traction.\",  # Tests rare and hyphenated words\n",
        "    \"🔥 Emojis & special #hashtags are tricky to tokenize!\"  # Tests handling of emojis and special characters\n",
        "]\n"
      ],
      "metadata": {
        "id": "6i6pQouzzwTc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word-Based Tokenizers\n",
        "\n",
        "- Splits text into words based on spaces and punctuation.\n",
        "- Easy to understand but inefficient for handling rare words.\n",
        "- Issue: Large vocabulary size\n",
        "- Issue: Ddifficulty handling out-of-vocabulary (OOV) words.\n",
        "\n",
        "**Use Cases**\n",
        "- Early NLP models like Bag-of-Words (BoW)\n",
        "- Traditional rule-based NLP"
      ],
      "metadata": {
        "id": "1FHJ7WAytjcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word-based Tokenization Example\n",
        "from transformers import BasicTokenizer\n",
        "bt = BasicTokenizer()\n",
        "word_based_tokens = list(map(bt.tokenize, sentences))\n",
        "\n",
        "print(f'Example: {word_based_tokens[0]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onGRR1dwuFwl",
        "outputId": "a01ac2c3-3341-4a8f-82cb-5f923825bc79"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example: ['i', 'can', \"'\", 't', 'believe', 'it', \"'\", 's', 'already', 'april', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Whitespace Tokenizers\n",
        "- Splits text by spaces only, ignoring punctuation.\n",
        "- Issues: Doesn't differentiate between punctuation and words, leading to inconsistencies.\n",
        "\n",
        "**Use Cases**\n",
        "- Very basic NLP preprocessing\n",
        "- Rule-based text processing"
      ],
      "metadata": {
        "id": "orh03brdwMT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Whitespace Tokenizer Example\n",
        "# Hugging Face does not provide a whitespace tokenizer\n",
        "\n",
        "# Define a simple whitespace tokenizer\n",
        "def whitespace_tokenizer(text):\n",
        "    return text.split()  # Split on spaces only\n",
        "\n",
        "whitespace_tokens = list(map(whitespace_tokenizer, sentences))\n",
        "\n",
        "print(f'Example: {whitespace_tokens[0]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPt5mpzU7gut",
        "outputId": "8f3b64b0-d890-4017-e983-a89d3d18f2cc"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example: ['I', \"can't\", 'believe', \"it's\", 'already', 'April!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Character-Based Tokenizers\n",
        "- Splits text into individual characters.\n",
        "- Handles rare words well because every character exists in the vocabulary.\n",
        "- Issue: Very long sequences\n",
        "- Issue: Harder for models to learn meaning from individual characters.\n",
        "\n",
        "**Use Cases**\n",
        "- OCR (Optical Character Recognition)\n",
        "- Some speech recognition models\n",
        "- Character-level RNNs and CNNs\n",
        "\n"
      ],
      "metadata": {
        "id": "bL8u1DL9uH91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Character-Based Tokenizer Example\n",
        "# Hugging Face does not provide a whitespace tokenizer\n",
        "\n",
        "# Define a simple Character-Based tokenizer\n",
        "def character_based_tokenizer(text):\n",
        "    return list(text)\n",
        "\n",
        "character_tokens = list(map(character_based_tokenizer, sentences))\n",
        "\n",
        "print(f'Example: {character_tokens[0]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNAzpk_u9n0O",
        "outputId": "22204c8d-79f6-4a27-f483-61abc03081ec"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example: ['I', ' ', 'c', 'a', 'n', \"'\", 't', ' ', 'b', 'e', 'l', 'i', 'e', 'v', 'e', ' ', 'i', 't', \"'\", 's', ' ', 'a', 'l', 'r', 'e', 'a', 'd', 'y', ' ', 'A', 'p', 'r', 'i', 'l', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subword Tokenizers\n",
        "- Most Popular in Modern NLP\n",
        "- Breaks words into meaningful subwords to balance vocabulary size and efficiency.\n",
        "- Can handle rare and compound words effectively.\n",
        "- Issue: Requires training a tokenizer model before use.\n",
        "\n",
        "### Byte-Pair Encoding (BPE) Subword Tokenizer\n",
        "- Merges frequently occurring character pairs to form subwords.\n",
        "- Used in GPT-2, RoBERTa, and SentencePiece architectures."
      ],
      "metadata": {
        "id": "jITI8Ismv1aI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GPT-2's pretrained BPE tokenizer\n",
        "bpe_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "print(f'Tokenizer Name: {bpe_tokenizer.__class__.__name__}')\n",
        "print(f'Tokenizer Type: {bpe_tokenizer.backend_tokenizer.model.__class__.__name__}')\n",
        "\n",
        "bpe_tokens = list(map(bpe_tokenizer.tokenize, sentences))\n",
        "\n",
        "print(f'Example: {bpe_tokens[0]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGBHJMFXyQK2",
        "outputId": "ba9fb8d3-15e9-4ed6-e574-5879ebc80d09"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer Name: GPT2TokenizerFast\n",
            "Tokenizer Type: BPE\n",
            "Example: ['I', 'Ġcan', \"'t\", 'Ġbelieve', 'Ġit', \"'s\", 'Ġalready', 'ĠApril', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WordPiece Subword Tokenizer\n",
        "\n",
        "- Similar to BPE but uses a probabilistic model to decide merges.\n",
        "- Used in BERT, DistilBERT."
      ],
      "metadata": {
        "id": "CzLU22KXyQnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Bert Base Cased's pretrained Wordpiece tokenizer\n",
        "wp_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "print(f'Tokenizer Name: {wp_tokenizer.__class__.__name__}')\n",
        "print(f'Tokenizer Type: {wp_tokenizer.backend_tokenizer.model.__class__.__name__}')\n",
        "\n",
        "wp_tokens = list(map(wp_tokenizer.tokenize, sentences))\n",
        "\n",
        "print(f'Example: {wp_tokens[0]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wmRoVNcymRv",
        "outputId": "91cdd0f2-70ec-49e6-8907-61222d238d1d"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer Name: BertTokenizerFast\n",
            "Tokenizer Type: WordPiece\n",
            "Example: ['I', 'can', \"'\", 't', 'believe', 'it', \"'\", 's', 'already', 'April', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unigram Language Model\n",
        "- Uses probabilities to split words into subwords differently based on the dataset.\n",
        "- Used in ALBERT, T5, XLNet."
      ],
      "metadata": {
        "id": "vhtOpZW5ymeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load T5's pretrained Unigram tokenizer\n",
        "ulm_tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
        "\n",
        "print(f'Tokenizer Name: {ulm_tokenizer.__class__.__name__}')\n",
        "print(f'Tokenizer Type: {ulm_tokenizer.backend_tokenizer.model.__class__.__name__}')\n",
        "\n",
        "ulm_tokens = list(map(ulm_tokenizer.tokenize, sentences))\n",
        "\n",
        "print(f'Example: {ulm_tokens[0]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIZjPq4Cyl7I",
        "outputId": "dc0d9edb-c0bf-4d27-b982-bc63b167f102"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer Name: T5TokenizerFast\n",
            "Tokenizer Type: Unigram\n",
            "Example: ['▁I', '▁can', \"'\", 't', '▁believe', '▁it', \"'\", 's', '▁already', '▁April', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Print examples side-by-side"
      ],
      "metadata": {
        "id": "Agrn-x3cKSTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index, value in enumerate(sentences):\n",
        "    print(f'Sentence: {value}')\n",
        "    print('\\nWord-based Tokenization:')\n",
        "    print('----------------------------------')\n",
        "    print(word_based_tokens[index])\n",
        "    print('\\nWhitespace Tokenization:')\n",
        "    print('----------------------------------')\n",
        "    print(whitespace_tokens[index])\n",
        "    print('\\nCharacter-based Tokenization:')\n",
        "    print('----------------------------------')\n",
        "    print(character_tokens[index])\n",
        "    print('\\nBPE Subword Tokenization:')\n",
        "    print('----------------------------------')\n",
        "    print(bpe_tokens[index])\n",
        "    print('\\nWordpiece Subword Tokenization:')\n",
        "    print('----------------------------------')\n",
        "    print(wp_tokens[index])\n",
        "    print('\\nUnigram Subword Tokenization:')\n",
        "    print('----------------------------------')\n",
        "    print(ulm_tokens[index])\n",
        "    print('\\n=======================================================\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poqEAv0A7VHT",
        "outputId": "93e6b37c-b76a-4919-bf5d-2e63ee3d6090"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: I can't believe it's already April!\n",
            "\n",
            "Word-based Tokenization:\n",
            "----------------------------------\n",
            "['i', 'can', \"'\", 't', 'believe', 'it', \"'\", 's', 'already', 'april', '!']\n",
            "\n",
            "Whitespace Tokenization:\n",
            "----------------------------------\n",
            "['I', \"can't\", 'believe', \"it's\", 'already', 'April!']\n",
            "\n",
            "Character-based Tokenization:\n",
            "----------------------------------\n",
            "['I', ' ', 'c', 'a', 'n', \"'\", 't', ' ', 'b', 'e', 'l', 'i', 'e', 'v', 'e', ' ', 'i', 't', \"'\", 's', ' ', 'a', 'l', 'r', 'e', 'a', 'd', 'y', ' ', 'A', 'p', 'r', 'i', 'l', '!']\n",
            "\n",
            "BPE Subword Tokenization:\n",
            "----------------------------------\n",
            "['I', 'Ġcan', \"'t\", 'Ġbelieve', 'Ġit', \"'s\", 'Ġalready', 'ĠApril', '!']\n",
            "\n",
            "Wordpiece Subword Tokenization:\n",
            "----------------------------------\n",
            "['I', 'can', \"'\", 't', 'believe', 'it', \"'\", 's', 'already', 'April', '!']\n",
            "\n",
            "Unigram Subword Tokenization:\n",
            "----------------------------------\n",
            "['▁I', '▁can', \"'\", 't', '▁believe', '▁it', \"'\", 's', '▁already', '▁April', '!']\n",
            "\n",
            "=======================================================\n",
            "\n",
            "Sentence: The quick brown fox jumps over the lazy dog.\n",
            "\n",
            "Word-based Tokenization:\n",
            "----------------------------------\n",
            "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
            "\n",
            "Whitespace Tokenization:\n",
            "----------------------------------\n",
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.']\n",
            "\n",
            "Character-based Tokenization:\n",
            "----------------------------------\n",
            "['T', 'h', 'e', ' ', 'q', 'u', 'i', 'c', 'k', ' ', 'b', 'r', 'o', 'w', 'n', ' ', 'f', 'o', 'x', ' ', 'j', 'u', 'm', 'p', 's', ' ', 'o', 'v', 'e', 'r', ' ', 't', 'h', 'e', ' ', 'l', 'a', 'z', 'y', ' ', 'd', 'o', 'g', '.']\n",
            "\n",
            "BPE Subword Tokenization:\n",
            "----------------------------------\n",
            "['The', 'Ġquick', 'Ġbrown', 'Ġfox', 'Ġjumps', 'Ġover', 'Ġthe', 'Ġlazy', 'Ġdog', '.']\n",
            "\n",
            "Wordpiece Subword Tokenization:\n",
            "----------------------------------\n",
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
            "\n",
            "Unigram Subword Tokenization:\n",
            "----------------------------------\n",
            "['▁The', '▁quick', '▁brown', '▁', 'fox', '▁jump', 's', '▁over', '▁the', '▁lazy', '▁dog', '.']\n",
            "\n",
            "=======================================================\n",
            "\n",
            "Sentence: Artificial intelligence (AI) is evolving rapidly!\n",
            "\n",
            "Word-based Tokenization:\n",
            "----------------------------------\n",
            "['artificial', 'intelligence', '(', 'ai', ')', 'is', 'evolving', 'rapidly', '!']\n",
            "\n",
            "Whitespace Tokenization:\n",
            "----------------------------------\n",
            "['Artificial', 'intelligence', '(AI)', 'is', 'evolving', 'rapidly!']\n",
            "\n",
            "Character-based Tokenization:\n",
            "----------------------------------\n",
            "['A', 'r', 't', 'i', 'f', 'i', 'c', 'i', 'a', 'l', ' ', 'i', 'n', 't', 'e', 'l', 'l', 'i', 'g', 'e', 'n', 'c', 'e', ' ', '(', 'A', 'I', ')', ' ', 'i', 's', ' ', 'e', 'v', 'o', 'l', 'v', 'i', 'n', 'g', ' ', 'r', 'a', 'p', 'i', 'd', 'l', 'y', '!']\n",
            "\n",
            "BPE Subword Tokenization:\n",
            "----------------------------------\n",
            "['Art', 'ificial', 'Ġintelligence', 'Ġ(', 'AI', ')', 'Ġis', 'Ġevolving', 'Ġrapidly', '!']\n",
            "\n",
            "Wordpiece Subword Tokenization:\n",
            "----------------------------------\n",
            "['Art', '##ific', '##ial', 'intelligence', '(', 'AI', ')', 'is', 'evolving', 'rapidly', '!']\n",
            "\n",
            "Unigram Subword Tokenization:\n",
            "----------------------------------\n",
            "['▁Artificial', '▁intelligence', '▁(', 'AI', ')', '▁is', '▁evolving', '▁rapidly', '!']\n",
            "\n",
            "=======================================================\n",
            "\n",
            "Sentence: Unbelievably, the anti-establishment movement gained traction.\n",
            "\n",
            "Word-based Tokenization:\n",
            "----------------------------------\n",
            "['unbelievably', ',', 'the', 'anti', '-', 'establishment', 'movement', 'gained', 'traction', '.']\n",
            "\n",
            "Whitespace Tokenization:\n",
            "----------------------------------\n",
            "['Unbelievably,', 'the', 'anti-establishment', 'movement', 'gained', 'traction.']\n",
            "\n",
            "Character-based Tokenization:\n",
            "----------------------------------\n",
            "['U', 'n', 'b', 'e', 'l', 'i', 'e', 'v', 'a', 'b', 'l', 'y', ',', ' ', 't', 'h', 'e', ' ', 'a', 'n', 't', 'i', '-', 'e', 's', 't', 'a', 'b', 'l', 'i', 's', 'h', 'm', 'e', 'n', 't', ' ', 'm', 'o', 'v', 'e', 'm', 'e', 'n', 't', ' ', 'g', 'a', 'i', 'n', 'e', 'd', ' ', 't', 'r', 'a', 'c', 't', 'i', 'o', 'n', '.']\n",
            "\n",
            "BPE Subword Tokenization:\n",
            "----------------------------------\n",
            "['Un', 'bel', 'iev', 'ably', ',', 'Ġthe', 'Ġanti', '-', 'establishment', 'Ġmovement', 'Ġgained', 'Ġtraction', '.']\n",
            "\n",
            "Wordpiece Subword Tokenization:\n",
            "----------------------------------\n",
            "['Un', '##bel', '##ie', '##va', '##bly', ',', 'the', 'anti', '-', 'establishment', 'movement', 'gained', 'traction', '.']\n",
            "\n",
            "Unigram Subword Tokenization:\n",
            "----------------------------------\n",
            "['▁Un', 'be', 'lie', 'v', 'ably', ',', '▁the', '▁anti', '-', 'est', 'abl', 'ish', 'ment', '▁movement', '▁gained', '▁', 'traction', '.']\n",
            "\n",
            "=======================================================\n",
            "\n",
            "Sentence: 🔥 Emojis & special #hashtags are tricky to tokenize!\n",
            "\n",
            "Word-based Tokenization:\n",
            "----------------------------------\n",
            "['🔥', 'emojis', '&', 'special', '#', 'hashtags', 'are', 'tricky', 'to', 'tokenize', '!']\n",
            "\n",
            "Whitespace Tokenization:\n",
            "----------------------------------\n",
            "['🔥', 'Emojis', '&', 'special', '#hashtags', 'are', 'tricky', 'to', 'tokenize!']\n",
            "\n",
            "Character-based Tokenization:\n",
            "----------------------------------\n",
            "['🔥', ' ', 'E', 'm', 'o', 'j', 'i', 's', ' ', '&', ' ', 's', 'p', 'e', 'c', 'i', 'a', 'l', ' ', '#', 'h', 'a', 's', 'h', 't', 'a', 'g', 's', ' ', 'a', 'r', 'e', ' ', 't', 'r', 'i', 'c', 'k', 'y', ' ', 't', 'o', ' ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'e', '!']\n",
            "\n",
            "BPE Subword Tokenization:\n",
            "----------------------------------\n",
            "['ðŁ', 'Ķ', '¥', 'ĠEm', 'oj', 'is', 'Ġ&', 'Ġspecial', 'Ġ#', 'hash', 'tags', 'Ġare', 'Ġtricky', 'Ġto', 'Ġtoken', 'ize', '!']\n",
            "\n",
            "Wordpiece Subword Tokenization:\n",
            "----------------------------------\n",
            "['[UNK]', 'Em', '##o', '##ji', '##s', '&', 'special', '#', 'has', '##hta', '##gs', 'are', 'trick', '##y', 'to', 'token', '##ize', '!']\n",
            "\n",
            "Unigram Subword Tokenization:\n",
            "----------------------------------\n",
            "['▁', '🔥', '▁Em', 'oji', 's', '▁', '&', '▁special', '▁#', 'h', 'ash', 'tags', '▁are', '▁tricky', '▁to', '▁token', 'ize', '!']\n",
            "\n",
            "=======================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q6B_Vod8IkkW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}